,instructions,answers
1,Explain the ultimate objective of creating and deploying a personalized AI writing assistant.,"The end goal? Build and deploy your own LLM twin. Learn production ML by building and deploying an end-to-end production grade LLM system, moving beyond isolated scripts or Notebooks."
13,Detail the cost expectations for running specific pipelines.,"From our tests, it is anticipated that running the fine-tuning and inference pipelines will cost 15. For other necessary serverless tools, such as Qdrant, Comet, and Opik, the plan is to utilize their free versions."
7,Explain how a fine-tuned language model is deployed and utilized for inference.,"The fine-tuned LLM is loaded from the model registry and quantized before being deployed, often to an AWS SageMaker REST API. Prompts sent to the model are enhanced using RAG techniques. The model's performance is monitored via tools like Opik, and it can be integrated with interfaces such as a Gradio UI."
6,Outline the steps involved in preparing and selecting a fine-tuned language model.,"A custom instruction dataset is first created based on digital data. An LLM is subsequently fine-tuned utilizing tools such as Unsloth, AWS SageMaker, and Comet ML for experiment tracking. The resulting LLMs undergo evaluation with tools like Opik, and the best-performing model is saved to a model registry, for instance, the Hugging Face model registry."
9,Detail the availability and potential expenses associated with learning materials that include runnable code.,"Resources like articles and open-source code are provided completely free and are intended to stay that way. However, executing the code might lead to expenses, as it involves cloud tools like AWS, which operates on a pay-as-you-go pricing model, potentially generating costs."
3,Outline the typical MLOps practices utilized in managing LLM systems.,"You will learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning."
12,Outline the approach to designing the LLM project system.,"We will dig into the LLM project system design. This includes presenting all our architectural decisions, particularly concerning the design of the data collection pipeline for social media data, and showing how the 3-pipeline architecture was applied."
8,Describe the function of Opik in the language model lifecycle.,"Opik plays a role in both the training and inference stages of a language model. During training, it is utilized to evaluate the fine-tuned LLMs to identify the best model. Post-deployment, during inference, Opik serves as a tool to monitor the performance of the operational LLM."
5,Describe the process of handling messages for ingestion into a vector database in real time.,"Messages are consumed from a queue using a streaming pipeline, like one powered by Bytewax. This feature pipeline then cleans, chunks, and embeds each message before loading the processed data into a vector DB, ensuring real-time updates."
4,Detail the process of the data collection pipeline within an LLM twin's architecture.,"The data collection pipeline crawls your digital data from various social media platforms. It cleans, normalizes and loads the data to a NoSQL DB through a series of ETL pipelines."
0,Define an AI character designed to replicate a specific writing style.,"It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM."
11,Explain the 3-pipeline design concept.,The 3-pipeline design is an architectural approach. We will explain what this design is and subsequently demonstrate how it is applied within the context of a standard ML system and specifically to our LLM microservices.
