{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "# DB = CLIENT.antonio\n",
    "# PEOPLE = DB.people\n",
    "# DOCUMENTS = DB.documents\n",
    "# for person in PEOPLE.find():\n",
    "#     print(person)\n",
    "\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# import os\n",
    "#\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"https://b7fce096-1c85-492d-b757-1724657c30f2.eu-west-2-0.aws.cloud.qdrant.\"\n",
    "#     \"io:6333\",\n",
    "#     api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# query_embedding = np.random.rand(384).tolist()  # Simula un embedding de consulta\n",
    "#\n",
    "# search_results = qdrant_client.search(\n",
    "#     collection_name=\"llms\",\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=3,  # Devuelve los 3 mÃ¡s similares\n",
    "# )\n",
    "#\n",
    "# for result in search_results:\n",
    "#     print(f\"ID: {result.id}, Score: {result.score}, Data: {result.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8712/1224811186.py:9: UserWarning: WARNING: Unsloth should be imported before trl, transformers, peft to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel, is_bfloat16_supported\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TextStreamer, TrainingArguments\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel, is_bfloat16_supported\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_templates\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_chat_template\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Escritorio/llms/code/llm-engineering/.venv/lib/python3.12/site-packages/unsloth/__init__.py:93\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;66;03m# First check if CUDA is available ie a NVIDIA GPU is seen\u001b[39;00m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.cuda.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Fix Xformers performance issues since 0.0.25\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth: No NVIDIA GPU found? Unsloth currently only supports GPUs!"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Script to perform fine-tuning in the LLM.\n",
    "\"\"\"\n",
    "\n",
    "from typing import Any\n",
    "from datasets import Dataset, load_dataset\n",
    "from transformers import TextStreamer, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "from peft import PeftModel\n",
    "from unsloth import PatchDPOTrainer\n",
    "\n",
    "from src.sft.constants import (\n",
    "    MODEL_NAME,\n",
    "    MAX_SEQ_LENGTH,\n",
    "    LOAD_IN_4_BIT,\n",
    "    LORA_RANK,\n",
    "    LORA_ALPHA,\n",
    "    LORA_DROPOUT,\n",
    "    TARGET_MODULES,\n",
    "    ALPACA_TEMPLATE,\n",
    "    OUTPUT_DIR,\n",
    "    CHAT_TEMPLATE,\n",
    "    LEARNING_RATE,\n",
    "    NUM_TRAIN_EPOCHS,\n",
    "    PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "    GRADIENT_ACCUMULATION_STEPS,\n",
    "    TEST_SIZE,\n",
    "    OPTIM,\n",
    "    LR_SCHEDULER_TYPE,\n",
    "    WARMUP_STEPS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_model() -> tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads the model and tokenizer we will fine-tune.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Model and tokenizer to fine-tune.\n",
    "    \"\"\"\n",
    "\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=LOAD_IN_4_BIT,\n",
    "    )\n",
    "\n",
    "    model = FastLanguageModel.get_peft_model(\n",
    "        model,\n",
    "        r=LORA_RANK,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        target_modules=TARGET_MODULES,\n",
    "    )\n",
    "\n",
    "    tokenizer = get_chat_template(\n",
    "        tokenizer,\n",
    "        chat_template=CHAT_TEMPLATE,\n",
    "    )\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def finetune() -> tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Fine-tunes the model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Fine-tuned model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    model, tokenizer = _load_model()\n",
    "    eos_token = tokenizer.eos_token\n",
    "    print(f\"Setting EOS_TOKEN to {eos_token}\")\n",
    "\n",
    "    def format_samples_sft(examples: Dataset) -> dict[str, list[str]]:\n",
    "        \"\"\"\n",
    "        Function to include EOS token at the end of the instruction and output.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        examples : HF dataset.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Modified text.\n",
    "        \"\"\"\n",
    "\n",
    "        text = []\n",
    "\n",
    "        for instruction, output in zip(\n",
    "            examples[\"instructions\"], examples[\"answers\"], strict=False\n",
    "        ):\n",
    "            message = ALPACA_TEMPLATE.format(instruction, output) + eos_token\n",
    "            text.append(message)\n",
    "\n",
    "        return {\"text\": text}\n",
    "\n",
    "    # dataset = load_dataset(\"mlabonne/FineTome-Alpaca-100k\")\n",
    "    dataset = load_dataset(\"csv\", data_files=\"data/datasets/df_sft.csv\", split=\"train\")\n",
    "    print(f\"Loaded dataset with {len(dataset)} samples.\")\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        format_samples_sft, batched=True, remove_columns=dataset.column_names\n",
    "    )\n",
    "    dataset = dataset.train_test_split(test_size=TEST_SIZE)\n",
    "\n",
    "    print(\"Training dataset example:\")\n",
    "    print(dataset[\"train\"][0])\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        eval_dataset=dataset[\"test\"],\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        dataset_num_proc=2,\n",
    "        packing=True,\n",
    "        args=TrainingArguments(\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "            per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            fp16=not is_bfloat16_supported(),\n",
    "            bf16=is_bfloat16_supported(),\n",
    "            logging_steps=1,\n",
    "            optim=OPTIM,\n",
    "            weight_decay=0.01,\n",
    "            lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "            per_device_eval_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "            warmup_steps=WARMUP_STEPS,\n",
    "            output_dir=OUTPUT_DIR,\n",
    "            report_to=\"comet_ml\",\n",
    "            seed=0,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def inference(\n",
    "    model: Any,\n",
    "    tokenizer: Any,\n",
    "    prompt: str = \"Write a paragraph to introduce supervised fine-tuning.\",\n",
    "    max_new_tokens: int = 256,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generates a response of the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model          : LLM.\n",
    "    tokenizer      : Tokenizer.\n",
    "    prompt         : Prompt.\n",
    "    max_new_tokens : Maximum number of tokens generated.\n",
    "    \"\"\"\n",
    "\n",
    "    model = FastLanguageModel.for_inference(model)\n",
    "    message = ALPACA_TEMPLATE.format(prompt, \"\")\n",
    "    inputs = tokenizer([message], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    _ = model.generate(\n",
    "        **inputs, streamer=text_streamer, max_new_tokens=max_new_tokens, use_cache=True\n",
    "    )\n",
    "\n",
    "\n",
    "def save_model(\n",
    "    model: Any,\n",
    "    tokenizer: Any,\n",
    "    output_dir: str = \"data/models\",\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Saves the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model      : Model.\n",
    "    tokenizer  : Tokenizer.\n",
    "    output_dir : Directory where the model weights will be saved.\n",
    "    \"\"\"\n",
    "\n",
    "    model.save_pretrained_merged(\n",
    "        output_dir, tokenizer, save_method=\"lora\"\n",
    "    )  # \"merged_16bit\")\n",
    "\n",
    "\n",
    "def load_model(model_path: str = \"data/models\") -> tuple[Any, Any]:\n",
    "    \"\"\"\n",
    "    Loads the fine-tuned model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_path : Path where the weights of the model are saved.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Fine-tuned model and tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    base_model, trained_tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name=MODEL_NAME,\n",
    "        max_seq_length=MAX_SEQ_LENGTH,\n",
    "        load_in_4bit=LOAD_IN_4_BIT,\n",
    "    )\n",
    "    trained_model = PeftModel.from_pretrained(base_model, model_path)\n",
    "\n",
    "    return trained_model, trained_tokenizer\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    ft_model, ft_tokenizer = finetune()\n",
    "    inference(ft_model, ft_tokenizer)\n",
    "    save_model(ft_model, ft_tokenizer)\n",
    "    saved_model, saved_tokenizer = load_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
