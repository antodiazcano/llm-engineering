{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "# DB = CLIENT.antonio\n",
    "# PEOPLE = DB.people\n",
    "# DOCUMENTS = DB.documents\n",
    "# for person in PEOPLE.find():\n",
    "#     print(person)\n",
    "\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# import os\n",
    "#\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"https://b7fce096-1c85-492d-b757-1724657c30f2.eu-west-2-0.aws.cloud.qdrant.\"\n",
    "#     \"io:6333\",\n",
    "#     api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# query_embedding = np.random.rand(384).tolist()  # Simula un embedding de consulta\n",
    "#\n",
    "# search_results = qdrant_client.search(\n",
    "#     collection_name=\"llms\",\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=3,  # Devuelve los 3 mÃ¡s similares\n",
    "# )\n",
    "#\n",
    "# for result in search_results:\n",
    "#     print(f\"ID: {result.id}, Score: {result.score}, Data: {result.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def get_documents() -> list[str]:\n",
    "    client: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "    db = client.antonio\n",
    "    docs = db.documents\n",
    "    documents = []\n",
    "    for doc in docs.find():\n",
    "        documents.append(doc[\"content\"][\"Content\"])\n",
    "    return documents\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def extract_chunks(\n",
    "    documents: list[str], min_length: int = 1000, max_length: int = 2000\n",
    ") -> list[str]:\n",
    "    answers = []\n",
    "    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\"\n",
    "    for document in documents:\n",
    "        cleaned_article = clean_text(document)\n",
    "        sentences = re.split(sentence_pattern, cleaned_article)\n",
    "        current_chunk = \"\"\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            if len(current_chunk) + len(sentence) <= max_length:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                if len(current_chunk) >= min_length:\n",
    "                    answers.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "        if len(current_chunk) >= min_length:\n",
    "            answers.append(current_chunk.strip())\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import google.generativeai as genai\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "\n",
    "def generate_instruction_answer_pairs(answer):\n",
    "    prompt = f\"\"\"Based on the following extract, generate five instruction-answer \\\n",
    "    pairs. Each instruction must ask to write about a specific topic contained in the \\\n",
    "    context. Each answer must provide a relevant paragraph based on the information \\\n",
    "    found in the context. Only use concepts from the context to generate the \\\n",
    "    instructions. Instructions must never explicitly mention a context, a system, a \\\n",
    "    course, or an extract. Instructions must be self-contained and general. Answers \\\n",
    "    must imitate the writing style of the context.\n",
    "    \n",
    "    Example instruction: Explain the concept of an LLM Twin.\n",
    "    Example answer: An LLM Twin is essentially an AI character that mimics your \\\n",
    "    writing style, personality, and voice. It's designed to write just like you by \\\n",
    "    incorporating these elements into a language model. The idea is to create a \\\n",
    "    digital replica of your writing habits using advanced AI techniques.\n",
    "    \n",
    "    Provide your response in JSON format with the following structure:\n",
    "    {{\n",
    "        \"instruction_answer_pairs\": [\n",
    "            {{\"instruction\": \"...\", \"answer\": \"...\"}},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Extract:\n",
    "    {answer}\n",
    "    \"\"\"\n",
    "\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "    except ResourceExhausted:\n",
    "        print(\"WARNING: Requests per minute exceeded!\")\n",
    "        return []\n",
    "\n",
    "    json_str = response.text.strip(\"```json\\n\").strip(\"```\").strip()\n",
    "    json_data = json.loads(json_str)\n",
    "    pairs = [\n",
    "        (pair[\"instruction\"], pair[\"answer\"])\n",
    "        for pair in json_data[\"instruction_answer_pairs\"]\n",
    "    ]\n",
    "\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(out_path: str = \"\") -> None:\n",
    "    # Extract documents, generate chunks and obtain pairs\n",
    "    documents = get_documents()\n",
    "    chunks = extract_chunks(documents)\n",
    "    instruction_answer_pairs = []\n",
    "    # The free plan limit is 5 requests per minute, so may be there are some errors\n",
    "    for chunk in chunks:\n",
    "        instruction_answer_pairs += generate_instruction_answer_pairs(chunk)\n",
    "    instructions, answers = zip(*instruction_answer_pairs)\n",
    "\n",
    "    # Save dfs\n",
    "    df = pd.DataFrame({\"instructions\": instructions, \"answers\": answers})\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "    df_train.to_csv(f\"{out_path}df_train.csv\")\n",
    "    df_test.to_csv(f\"{out_path}df_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Requests per minute exceeded!\n",
      "WARNING: Requests per minute exceeded!\n",
      "WARNING: Requests per minute exceeded!\n",
      "WARNING: Requests per minute exceeded!\n",
      "[(\"Describe the concept of an AI character designed to replicate a person's writing.\", \"This type of AI character is designed to write like a specific individual by incorporating their unique style, personality, and voice directly into a large language model (LLM). The goal is to create an AI that effectively mirrors the person's writing patterns and characteristics.\"), ('What are the core components involved in creating a production-ready AI replica?', 'Building a production-ready AI replica involves integrating several key technologies and practices. You would typically utilize Large Language Models (LLMs), vector databases (vector DBs), and adhere to LLMOps good practices throughout the design, training, and deployment phases.'), ('What are the benefits of building and deploying an end-to-end production-grade LLM system?', 'Building and deploying a complete, production-grade LLM system allows you to move beyond isolated scripts or notebooks. It provides practical experience in production Machine Learning, covering the entire lifecycle from initial design to final deployment.'), ('Detail the MLOps best practices relevant to developing LLM systems.', 'When building real-world LLM systems, leveraging MLOps best practices is crucial. Key practices include utilizing experiment trackers to monitor trials, employing model registries for version control, implementing prompt monitoring for performance analysis, and maintaining systematic versioning throughout the development process.'), ('Explain the role and process of a data collection pipeline for personalized AI.', \"A data collection pipeline serves to gather an individual's digital data, often crawling various social media platforms. This raw data then undergoes processing through a series of ETL (Extract, Transform, Load) pipelines, which clean and normalize it before loading it into a suitable storage system, such as a NoSQL database.\"), ('Describe how database changes can be processed through a queue into a vector database.', 'Using the CDC pattern, database changes are sent to a queue. A feature pipeline consumes messages from this queue through a Bytewax streaming pipeline. It then cleans, chunks, and embeds every message before loading it to a vector DB in real time.'), ('Outline the process of creating, tuning, and evaluating a custom language model.', \"The training pipeline creates a custom instruction dataset derived from digital data. An LLM is subsequently fine-tuned utilizing Unsloth, AWS SageMaker, and Comet ML's experiment tracker. Evaluation of the LLMs is conducted using Opik, and the superior model is preserved in the Hugging Face model registry.\"), ('Detail the steps involved in deploying and enhancing a fine-tuned language model for practical use.', 'The inference pipeline manages loading and quantizing the fine-tuned LLM from the model registry to the AWS SageMaker REST API. It enhances prompts through the use of RAG techniques. Furthermore, the LLM is monitored using Opik and can be connected to a Gradio UI.'), ('List some serverless tools and their functions within an ML system architecture.', 'Within the system architecture, several serverless tools can be integrated: Comet ML serves as your experiment tracker, Qdrant functions as your vector DB, AWS SageMaker provides your ML infrastructure, and Opik acts as your prompt evaluation and monitoring tool.'), ('Discuss the cost considerations when running code involving cloud tools.', 'While foundational articles and code may be offered completely free and intended to remain so, executing the code could involve cloud tools. For instance, AWS employs a pay-as-you-go pricing plan, meaning running the code might generate additional costs.'), ('Describe the concept of building an LLM Twin.', 'Building an LLM Twin involves creating your own production-ready AI replica. This project serves as the foundation for understanding an end-to-end framework for LLM systems, beginning with the LLM twin concept and progressing through system design and implementation details.'), ('Explain the 3 pipeline architecture in system design.', 'The 3 pipeline architecture is a design approach applied to standard ML systems and specifically leveraged in LLM project system design. It informs architectural decisions, particularly regarding the design of elements like data collection pipelines for social media data and the structure of LLM microservices.'), ('Outline the key aspects of LLM project system design.', 'LLM project system design involves defining the overall architectural strategy. This includes applying the 3 pipeline architecture, making specific choices for designing components such as the data collection pipeline for social media data, and structuring the necessary LLM microservices to create a cohesive system.'), ('Describe the architectural approach for a data collection pipeline handling specific data types.', 'Architectural decisions are crucial when designing a data collection pipeline, for instance, one handling social media data. These decisions form part of the broader LLM system design and are guided by the application of the 3 pipeline architecture to integrate seamlessly with other LLM microservices.'), ('Discuss the steps following the initial system design presentation.', 'After introducing the core project and the LLM system design, the focus shifts towards practical implementation. Subsequent steps involve examining the code for each system component in detail, learning how to implement them, and understanding deployment procedures, such as deploying to AWS SageMaker.'), (\"Describe an AI character designed to mimic a specific person's writing.\", 'It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.'), ('Explain the value of building complete, deployable systems in machine learning development.', 'No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production grade LLM system.'), ('Outline the key capabilities gained when building a complex language model application.', 'You will learn how to architect and build a real-world LLM system from start to finish from data collection to deployment.'), ('List some best practices associated with MLOps.', 'You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.'), ('Describe the function of a data collection pipeline within a specific microservice architecture.', 'The data collection pipeline crawls your digital data from various social media platforms. It cleans, normalizes and loads the data to a NoSQL DB through a series of ETL pipelines.'), ('Describe the process of handling database changes for feature pipelines in real time.', 'Using the CDC pattern, database changes are sent to a queue. A feature pipeline consumes messages from this queue through a Bytewax streaming pipeline. It cleans, chunks, and embeds every message before loading it to a vector DB in real time.'), ('Outline the steps involved in training and evaluating a custom language model.', \"First, a custom instruction dataset is created based on your digital data. An LLM is then fine-tuned using tools like Unsloth, AWS SageMaker, and Comet ML's experiment tracker. Subsequently, the LLMs are evaluated using Opik, and the best performing model is saved to the Hugging Face model registry.\"), ('Explain how a fine-tuned language model is deployed and monitored for inference.', \"The inference pipeline loads and quantizes the fine-tuned LLM from the model registry to the AWS SageMaker REST API. Prompts are enhanced using RAG techniques. The LLM's performance is monitored using Opik, and it can be hooked to a Gradio UI for interaction.\"), ('Detail the key serverless tools integrated within the system architecture.', 'Within the architecture, composed of 4 microservices, several serverless tools are integrated. These include Comet ML as the experiment tracker, Qdrant as the vector DB, AWS SageMaker providing the ML infrastructure, and Opik serving as the prompt evaluation and monitoring tool.'), ('Describe the intended audience and necessary prerequisites for learning about engineering production-ready LLM systems.', 'The intended audience consists of MLEs, DEs, DSs, or SWEs who wish to learn the engineering of production-ready LLM and RAG systems using LLMOps best practices. Prerequisites include a basic knowledge of Python and ML.')]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'Dataframe'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mgenerate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mgenerate_dataset\u001b[39m\u001b[34m(out_path)\u001b[39m\n\u001b[32m     10\u001b[39m instructions, answers = \u001b[38;5;28mzip\u001b[39m(*instruction_answer_pairs)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Save dfs\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataframe\u001b[49m({\u001b[33m\"\u001b[39m\u001b[33minstructions\u001b[39m\u001b[33m\"\u001b[39m: instructions, \u001b[33m\"\u001b[39m\u001b[33manswers\u001b[39m\u001b[33m\"\u001b[39m: answers})\n\u001b[32m     14\u001b[39m df_train, df_test = train_test_split(df, test_size=\u001b[32m0.2\u001b[39m)\n\u001b[32m     15\u001b[39m df_train.to_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mdf_train.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pandas' has no attribute 'Dataframe'"
     ]
    }
   ],
   "source": [
    "generate_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
