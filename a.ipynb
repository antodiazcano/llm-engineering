{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag_main.pre_retrieval import query_expansion, self_query\n",
    "\n",
    "query = \"What is RAG from Paul Iusztin?\"\n",
    "n = 3\n",
    "\n",
    "queries = query_expansion(query, n)\n",
    "metadata = self_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.rag_main.retrieval import top_k_matches\n",
    "\n",
    "nk_matches = []\n",
    "ids = []\n",
    "k = 2\n",
    "filters = {\"author\": metadata}\n",
    "\n",
    "for query in queries:\n",
    "    top_k = top_k_matches(query, k, filters=filters)\n",
    "    for result in top_k:\n",
    "        if result.id not in ids:\n",
    "            nk_matches.append(result.payload[\"chunk\"])\n",
    "            ids.append(result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "\n",
    "\n",
    "model = CrossEncoder()\n",
    "\n",
    "top_k = sorted(\n",
    "    [(chunk, float(model.predict([(chunk, query)])[0])) for chunk in nk_matches],\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"It cleans, chunks, and embeds every message and loads it to a vector DB in real-time.The training pipeline creates a custom instruction dataset based on your digital data. Fine-tune an LLM using Unsloth, AWS SageMaker, and Comet ML's experiment tracker. Evaluate the LLMs using Opik and save the best model to the Hugging Face model registry.The inference pipeline loads and quantizes the fine-tuned LLM from the model registry to the AWS SageMaker REST API. Enhance the prompts using RAG.\",\n",
       " 'It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the LLM Twin: Building Your Production-Ready AI Replica free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? No more isolated scripts or Notebooks!']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[match[0] for match in top_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-6.721551418304443"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(top_k[0][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textwrap\n",
    "\n",
    "expand_to_n = 3\n",
    "separator = \"#\"\n",
    "question = \"What is RAG?\"\n",
    "\n",
    "prompt = textwrap.dedent(\n",
    "    f\"\"\"\\\n",
    "    You are an AI language model assistant. Your task is to generate {expand_to_n} different\n",
    "    versions of the given user question to retrieve relevant documents from a vector database.\n",
    "    By generating multiple perspectives on the user question, your goal is to help the user\n",
    "    overcome some of the limitations of the distance-based similarity search.\n",
    "\n",
    "    Provide these alternative questions separated by '{separator}' and do not use nothing at\n",
    "    the start. For example, '1: ...' or 'Version 1: ...' is not correct. Just write the\n",
    "    alternative questions separated.\n",
    "\n",
    "    Original question: {question}.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are an AI language model assistant. Your task is to generate 3 different\\nversions of the given user question to retrieve relevant documents from a vector database.\\nBy generating multiple perspectives on the user question, your goal is to help the user\\novercome some of the limitations of the distance-based similarity search.\\n\\nProvide these alternative questions separated by '#' and do not use nothing at\\nthe start. For example, '1: ...' or 'Version 1: ...' is not correct. Just write the\\nalternative questions separated.\\n\\nOriginal question: What is RAG?.\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
