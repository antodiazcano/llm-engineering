{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "# DB = CLIENT.antonio\n",
    "# PEOPLE = DB.people\n",
    "# DOCUMENTS = DB.documents\n",
    "# for person in PEOPLE.find():\n",
    "#     print(person)\n",
    "\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# import os\n",
    "#\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"https://b7fce096-1c85-492d-b757-1724657c30f2.eu-west-2-0.aws.cloud.qdrant.\"\n",
    "#     \"io:6333\",\n",
    "#     api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# query_embedding = np.random.rand(384).tolist()  # Simula un embedding de consulta\n",
    "#\n",
    "# search_results = qdrant_client.search(\n",
    "#     collection_name=\"llms\",\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=3,  # Devuelve los 3 mÃ¡s similares\n",
    "# )\n",
    "#\n",
    "# for result in search_results:\n",
    "#     print(f\"ID: {result.id}, Score: {result.score}, Data: {result.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "\n",
    "def evaluate_answer(instruction: str, answer: str) -> dict:\n",
    "    prompt = f\"\"\"You are an expert judge. Please evaluate the quality of a given \\\n",
    "    answer to an instruction based on two criteria:\n",
    "    1. Accuracy: How factually correct is the information presented in the \\\n",
    "    answer? You are a technical expert in this topic.\n",
    "    2. Style: Is the tone and writing style appropriate for a blog post or social \\\n",
    "    media content? It should use simple but technical words\n",
    "    and avoid formal or academic language.\n",
    "\n",
    "    Accuracy scale:\n",
    "    1 (Poor): Contains factual errors or misleading information\n",
    "    2 (Good): Mostly accurate with minor errors or omissions\n",
    "    3 (Excellent): Highly accurate and comprehensive\n",
    "    \n",
    "    Style scale:\n",
    "    1 (Poor): Too formal, uses some overly complex words\n",
    "    2 (Good): Good balance of technical content and accessibility, but\n",
    "    still uses formal words and expressions\n",
    "    3 (Excellent): Perfectly accessible language for blog/social media,\n",
    "    uses simple but precise technical terms when necessary\n",
    "\n",
    "    Example of bad style: The Llama2 7B model constitutes a noteworthy\n",
    "    progression in the field of artificial intelligence, serving as the\n",
    "    successor to its predecessor, the original Llama architecture.\n",
    "    Example of excellent style: Llama2 7B outperforms the original Llama\n",
    "    model across multiple benchmarks.\n",
    "\n",
    "    Instruction: {instruction}\n",
    "\n",
    "    Answer: {answer}\n",
    "\n",
    "    Provide your evaluation in JSON format with the following structure:\n",
    "    {{\n",
    "        \"accuracy\": {{\n",
    "            \"analysis\": \"...\",\n",
    "            \"score\": 0\n",
    "        }},\n",
    "        \"style\": {{\n",
    "            \"analysis\": \"...\",\n",
    "            \"score\": 0\n",
    "        }}\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    client = InferenceClient(\n",
    "        provider=\"nebius\",\n",
    "        api_key=os.environ[\"HUGGINGFACE_KEY\"],\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"deepseek-ai/DeepSeek-R1\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    )\n",
    "    text = completion.choices[0].message.content\n",
    "    end_tag = \"</think>\"\n",
    "    think_pos = text.find(end_tag)\n",
    "    text_after_second_think = text[think_pos + len(end_tag) :]\n",
    "\n",
    "    return text_after_second_think.strip()\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"data/evaluation/answers.csv\")\n",
    "evaluations = [\n",
    "    evaluate_answer(instruction, answer)\n",
    "    for instruction, answer in zip(df[\"prompt\"], df[\"answer\"])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "accs_explanations = []\n",
    "styles = []\n",
    "styles_explanations = []\n",
    "\n",
    "for evaluation in evaluations:\n",
    "    clean_eval = evaluation.strip(\"`\").strip()\n",
    "    clean_eval = json.loads(clean_eval[4:].strip())\n",
    "    accuracies.append(clean_eval[\"accuracy\"][\"score\"])\n",
    "    accs_explanations.append(clean_eval[\"accuracy\"][\"analysis\"])\n",
    "    styles.append(clean_eval[\"style\"][\"score\"])\n",
    "    styles_explanations.append(clean_eval[\"style\"][\"analysis\"])\n",
    "\n",
    "df[\"accuracy\"] = accuracies\n",
    "df[\"style\"] = styles\n",
    "df[\"accuracy_explanation\"] = accs_explanations\n",
    "df[\"style_explanation\"] = styles_explanations\n",
    "\n",
    "df.to_csv(\"data/evaluation/evaluation.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>style</th>\n",
       "      <th>accuracy_explanation</th>\n",
       "      <th>style_explanation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>deepseek</td>\n",
       "      <td>Below is an instruction that describes a task....</td>\n",
       "      <td>The cost of running fine-tuning and inference ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The answer is highly accurate and comprehensiv...</td>\n",
       "      <td>The style is accessible and well-suited for bl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model                                             prompt  \\\n",
       "0  deepseek  Below is an instruction that describes a task....   \n",
       "\n",
       "                                              answer  accuracy  style  \\\n",
       "0  The cost of running fine-tuning and inference ...         3      3   \n",
       "\n",
       "                                accuracy_explanation  \\\n",
       "0  The answer is highly accurate and comprehensiv...   \n",
       "\n",
       "                                   style_explanation  \n",
       "0  The style is accessible and well-suited for bl...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
