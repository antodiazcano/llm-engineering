{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "# DB = CLIENT.antonio\n",
    "# PEOPLE = DB.people\n",
    "# DOCUMENTS = DB.documents\n",
    "# for person in PEOPLE.find():\n",
    "#     print(person)\n",
    "\n",
    "# import numpy as np\n",
    "# from qdrant_client import QdrantClient\n",
    "# import os\n",
    "#\n",
    "# qdrant_client = QdrantClient(\n",
    "#     url=\"https://b7fce096-1c85-492d-b757-1724657c30f2.eu-west-2-0.aws.cloud.qdrant.\"\n",
    "#     \"io:6333\",\n",
    "#     api_key=os.getenv(\"QDRANT_API_KEY\"),\n",
    "# )\n",
    "#\n",
    "# query_embedding = np.random.rand(384).tolist()  # Simula un embedding de consulta\n",
    "#\n",
    "# search_results = qdrant_client.search(\n",
    "#     collection_name=\"llms\",\n",
    "#     query_vector=query_embedding,\n",
    "#     limit=3,  # Devuelve los 3 mÃ¡s similares\n",
    "# )\n",
    "#\n",
    "# for result in search_results:\n",
    "#     print(f\"ID: {result.id}, Score: {result.score}, Data: {result.payload}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Script to generate train and test datasets in the format is instruction-answer pairs.\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "import google.generativeai as genai\n",
    "from google.api_core.exceptions import ResourceExhausted\n",
    "\n",
    "\n",
    "def _get_documents() -> list[str]:\n",
    "    \"\"\"\n",
    "    Obtains all documents from MongoDB.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    List with all stored documents.\n",
    "    \"\"\"\n",
    "\n",
    "    client: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "    db = client.antonio\n",
    "    docs = db.documents\n",
    "\n",
    "    documents = []\n",
    "    for doc in docs.find():\n",
    "        documents.append(doc[\"content\"][\"Content\"])\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "def _clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes non-alphanumeric characters except for apostrophes, periods, commas,\n",
    "    exclamation marks, and question marks. replace multiple consecutive whitespace\n",
    "    characters with a single space.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : Text to clean.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Cleaned text.\n",
    "    \"\"\"\n",
    "\n",
    "    text = re.sub(r\"[^\\w\\s.,!?']\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def _extract_chunks(\n",
    "    documents: list[str], min_length: int = 1000, max_length: int = 2000\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Divides the documents into multiple chunks.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    documents  : List with all the available documents.\n",
    "    min_length : Minimum length of the chunk.\n",
    "    max_length : Maximum length of the chunk.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Chunks obtained.\n",
    "    \"\"\"\n",
    "\n",
    "    answers = []\n",
    "    sentence_pattern = r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s\"\n",
    "\n",
    "    for document in documents:\n",
    "        cleaned_article = _clean_text(document)\n",
    "        sentences = re.split(sentence_pattern, cleaned_article)\n",
    "        current_chunk = \"\"\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if not sentence:\n",
    "                continue\n",
    "            if len(current_chunk) + len(sentence) <= max_length:\n",
    "                current_chunk += sentence + \" \"\n",
    "            else:\n",
    "                if len(current_chunk) >= min_length:\n",
    "                    answers.append(current_chunk.strip())\n",
    "                current_chunk = sentence + \" \"\n",
    "\n",
    "        if len(current_chunk) >= min_length:\n",
    "            answers.append(current_chunk.strip())\n",
    "\n",
    "    return answers\n",
    "\n",
    "\n",
    "def _generate_dpo_dataset(\n",
    "    extract: str, temperature: float = 0.7\n",
    ") -> list[tuple[str, str]]:\n",
    "    \"\"\"\n",
    "    Generates triplets of instruction and accepted and rejected answers given the\n",
    "    extract. Higher temperatures will give more diverse outputs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    extract     : Chunk of the document.\n",
    "    temperature : Temperature.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Triplets of extract, accepted and rejected answer.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"Based on the following extract, generate five instruction-answer \\\n",
    "    triples. Each triple should consist of:\n",
    "    1. An instruction asking about a specific topic in the context.\n",
    "    2. A generated answer that attempts to answer the instruction based on the context.\n",
    "    3. An extracted answer that is a relevant excerpt directly from the given context.\n",
    "    Instructions must be self-contained and general, without explicitly mentioning a \\\n",
    "    context, system, course, or extract.\n",
    "\n",
    "    Important:\n",
    "    - Ensure that the extracted answer is a verbatim copy from the context, including \\\n",
    "    all punctuation and apostrophes.\n",
    "    - Do not add any ellipsis (...) or [...] to indicate skipped text in the extracted \\\n",
    "    answer.\n",
    "    - If the relevant text is not continuous, use two separate sentences from the \\\n",
    "    context instead of skipping text.\n",
    "    \n",
    "    Provide your response in JSON format with the following structure:\n",
    "    {{\n",
    "        \"preference_triples\": [\n",
    "            {{\n",
    "                \"instruction\": \"...\",\n",
    "                \"generated_answer\": \"...\",\n",
    "                \"extracted_answer\":\"...\",\n",
    "                \"...\"\n",
    "            }},\n",
    "            ...\n",
    "        ]\n",
    "    }}\n",
    "    \n",
    "    Extract:\n",
    "    {extract}\n",
    "    \"\"\"\n",
    "\n",
    "    genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-pro-exp-03-25\")\n",
    "    try:\n",
    "        response = model.generate_content(\n",
    "            prompt, generation_config={\"temperature\": temperature}\n",
    "        )\n",
    "    except ResourceExhausted:\n",
    "        print(\"WARNING: Requests per minute exceeded!\")\n",
    "        return []\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = _get_documents()\n",
    "chunks = _extract_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = _generate_dpo_dataset(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'preference_triples': [{'instruction': \"What defines an AI character designed to replicate an individual's writing style?\",\n",
       "   'generated_answer': \"An AI character designed to replicate someone's writing is defined as one that incorporates the individual's specific style, personality, and voice into a large language model.\",\n",
       "   'extracted_answer': 'It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.'},\n",
       "  {'instruction': 'What skills are acquired upon completing a program focused on building a production-ready AI replica?',\n",
       "   'generated_answer': 'By completing the program, you will gain the skills to design, train, and deploy a production-ready AI replica, utilizing large language models, vector databases, and established LLMOps practices.',\n",
       "   'extracted_answer': 'By finishing the LLM Twin Building Your Production Ready AI Replica free course, you will learn how to design, train, and deploy a production ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.'},\n",
       "  {'instruction': 'What is the main benefit of learning through building and deploying a complete, production-grade language model system?',\n",
       "   'generated_answer': 'The primary advantage is moving beyond isolated development environments like scripts or notebooks to gain practical experience in production machine learning by constructing an end-to-end system.',\n",
       "   'extracted_answer': 'No more isolated scripts or Notebooks! Learn production ML by building and deploying an end to end production grade LLM system.'},\n",
       "  {'instruction': 'What knowledge is imparted regarding the creation of real-world language model systems?',\n",
       "   'generated_answer': 'You gain knowledge on architecting and building a complete real-world language model system, covering the process entirely from initial data collection through to final deployment.',\n",
       "   'extracted_answer': 'You will learn how to architect and build a real world LLM system from start to finish from data collection to deployment.'},\n",
       "  {'instruction': 'Identify some MLOps best practices that are taught in the context of building advanced AI systems.',\n",
       "   'generated_answer': 'Best practices taught include leveraging tools and techniques such as experiment trackers, model registries, prompt monitoring, and version control.',\n",
       "   'extracted_answer': 'You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.'}]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_str = response.text.strip(\"`json\\n\").strip(\"`\").strip()\n",
    "json_data = json.loads(json_str)\n",
    "triplets = [\n",
    "    (pair[\"instruction\"], pair[\"generated_answer\"], pair[\"extracted_answer\"])\n",
    "    for pair in json_data[\"preference_triples\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
