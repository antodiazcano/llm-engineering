{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "class MediumCrawler:\n",
    "    def __init__(self, scroll_limit: int = 5) -> None:\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        # options.add_argument(\"--no-sandbox\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        # options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        # options.add_argument(\"--log-level=3\")\n",
    "        # options.add_argument(\"--disable-popup-blocking\")\n",
    "        # options.add_argument(\"--disable-notifications\")\n",
    "        # options.add_argument(\"--disable-extensions\")\n",
    "        # options.add_argument(\"--disable-background-networking\")\n",
    "        # options.add_argument(\"--ignore-certificate-errors\")\n",
    "        # options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n",
    "        # options.add_argument(f\"--data-path={mkdtemp()}\")\n",
    "        # options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n",
    "        # options.add_argument(\"--remote-debugging-port=9226\")\n",
    "        # options.add_argument(r\"--profile-directory=Profile 2\")\n",
    "\n",
    "        self.scroll_limit = scroll_limit\n",
    "        self.driver = webdriver.Chrome(\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n",
    "        current_scroll = 0\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "            time.sleep(max(1 + np.random.uniform(1), np.random.normal(2, 2)))\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height or (\n",
    "                self.scroll_limit and current_scroll >= self.scroll_limit\n",
    "            ):\n",
    "                break\n",
    "            last_height = new_height\n",
    "            current_scroll += 1\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        logger.info(f\"Starting scrapping Medium article: {link}\")\n",
    "\n",
    "        self.driver.get(link)\n",
    "        self.scroll_page()\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n",
    "        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n",
    "\n",
    "        data = {\n",
    "            \"Title\": title[0].string if title else None,\n",
    "            \"Subtitle\": subtitle[0].string if subtitle else None,\n",
    "            \"Content\": soup.get_text(),\n",
    "        }\n",
    "\n",
    "        self.driver.close()\n",
    "        logger.info(f\"Successfully scraped and saved article: {link}\")\n",
    "\n",
    "        return {\"platfrom\": \"medium\", \"content\": data, \"link\": link, \"author\": \"temp\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 13:26:46.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mStarting scrapping Medium article: https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\u001b[0m\n",
      "\u001b[32m2025-03-19 13:26:55.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mSuccessfully scraped and saved article: https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'platfrom': 'medium',\n",
       " 'content': {'Title': 'Why The Focus Has Shifted from AI Agents to Agentic Workflows',\n",
       "  'Subtitle': 'We find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. But‚Ä¶',\n",
       "  'Content': \"Why The Focus Has Shifted from AI Agents to Agentic Workflows | by Cobus Greyling | Feb, 2025 | MediumOpen in appSign upSign inWriteSign upSign inHomeLibraryStoriesStatsTop highlight1Why The Focus Has Shifted from AI Agents to Agentic WorkflowsWe find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. But‚Ä¶Cobus Greyling¬∑Follow5 min read¬∑Feb 5, 20251.5K45ListenShare‚Ä¶there has a shift when it comes to commercial implementations with focus moving form AI Agents in favor of Agentic Workflows/Data Synthesis.Why is the focus moving away from AI Agents (for now)?Companies like Salesforce and Service made hard pivots to AI Agents, however, the stark reality of AI Agents is that the technology is not where it should be in terms of accuracy.If one looks past the marketing hype, and the great prototypes and demos there are of AI Agents, their accuracy is not yet suited for production.The ùóñùóπùóÆùòÇùó±ùó≤ ùóîùóú ùóîùó¥ùó≤ùóªùòÅ ùóñùóºùó∫ùóΩùòÇùòÅùó≤ùóø ùóúùóªùòÅùó≤ùóøùó≥ùóÆùó∞ùó≤ (ùóîùóñùóú) performance sits at 14% of that of human performance.The graph below from TheAgentFactory is an indication of where AI Agents sit in term of cost, steps and success rate. Notice how the success rate is around 20%.These figures are the stark reality of the current situation.SourceWith the recent release of the OpenAI Operator, computer use and web browser use accurate was reached of 30 to 50%, but this is still lagging below the 70%+ of human capability.SourceAnd added to this, there are interesting studies on how AI Agents with web browsing is susceptible and easy prey for attacks via nefarious pop-ups.There are two avenues for AI Agents to perform tasks like humans; the one is via a web browser (Webvoyager, OpenAI Operator, etc). The second is via the complete GUI of the OS (Anthropic).These approaches makes use of the GUI as the API for the AI Agents.Initial approaches looked at using individual APIs, but this is not practical due to the overhead of developing each and every API integration. Also, for many commercial applications there exists no API.Why the focus on Agentic WorkflowsEveryone is agreeing that modern knowledge work is broken, with various numbers being stated. One of the reports stated that workers spend 30% of their time searching for information.There is also a challenge for knowledge workers in answering complex questions and needing to synthesise information from various documents.Agentic Workflows (as shown in the image below) allows for reasoning and decomposing complex tasks into simpler sub-tasks and chaining these tasks together in a sequence.SourceBy executing this sequence, elements like observability, inspectability and discoverability are introduced.The synthesis of data will become increasingly important. Agentic Workflows for knowledge workers is one such example where work data and resources can be synthesised for the worker into one answer.Language Model providers are moving away from only offering the model, as such. But also extending into User Experience. Deep Research in ChatGPT is not a new model, but rather a new agentic capability within ChatGPT which conducts multi-step research on the internet for complex tasks. It accomplishes in tens of minutes what would take a human many hours.This is also a good example of how disparate sources of data is synthesised to answer a user question.I feel this is something LlamaIndex coined, the idea of Agentic RAG, where the notion is that synthesising data for an ‚Äúaudience of one‚Äù for a particular point in time will become important.In the coming months there will be immense focus on personal agentic workflows, information synthesis, something you could call desktop orchestration.Reasoning & Problem-SolvingModern AI models are increasingly integrating reasoning as a core feature, enabling them to tackle complex problems by breaking them down into manageable components.This shift is underpinned by an innovative approach that involves decomposing questions into smaller subsets, allowing the model to address each part systematically.By treating reasoning as an internal mechanism, these models can simulate human-like thought processes, enhancing their ability to provide accurate and nuanced responses.The decomposition strategy not only improves problem-solving efficiency but also fosters greater transparency in how conclusions are reached.As a result, users benefit from more interpretable outputs, bridging the gap between advanced computation and understandable decision-making.Initially users had to include reasoning traits in their prompt, instructing the model how to reason and decompose complex or compound tasks. And by giving examples via a few-show approach, for the model to emulate.In ClosingOrganisations must shift their focus from fixating on specific tools or trends ‚Äî like those that once branded themselves as RAG companies, Prompt Engineering playgrounds and more, and instead prioritise solving real-world business challenges.The world is moving forward at an unprecedented pace, with new technologies emerging almost daily, each promising to revolutionise industries.But, the true measure of innovation lies not in mastering the latest technology but in applying these advancements to create tangible value.Whether it‚Äôs improving customer experiences, streamlining operations, or addressing societal needs, the question remains, how can we leverage technology to deliver meaningful solutions?By adopting this mindset, businesses can future-proof themselves and ensure they remain relevant amid ever-changing tides of progress.Chief Evangelist @ Kore.ai | I‚Äôm passionate about exploring the intersection of AI and language. From Language Models, AI Agents to Agentic Applications, Development Frameworks & Data-Centric Productivity Tools, I share insights and ideas on how these technologies are shaping the future.COBUS GREYLINGWhere AI Meets Language | Language Models, AI Agents, Agentic Applications, Development Frameworks & Data-Centric‚Ä¶www.cobusgreyling.comSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for 5\\xa0$/monthAIArtificial IntelligenceMachine LearningLarge Language ModelsGenerative Ai Tools1.5K1.5K45FollowWritten by Cobus Greyling28K Followers¬∑0 FollowingI‚Äôm passionate about exploring the intersection of AI & language. www.cobusgreyling.comFollowResponses (45)Write a responseWhat are your thoughts?CancelRespondAlso publish to my profileSDLC CorpheFeb 13The shift from AI Agents to Agentic Workflows reflects a crucial reality. AI needs more than just automation; it requires structured reasoning and reliability. While AI Agents show promise, their accuracy challenges highlight why businesses are‚Ä¶more21Reply≈ûahin UtarFeb 12I expect a development towards self-integrating APIs where provider and consumer sides are deployed with agentic models that talk to each other and write and deploy api integration code (consume the other party's api, provide a web hook etc. based‚Ä¶more39ReplySten DiedenFeb 13Very interesting, sober and level headed in a context of otherwise incessant, hysterical hype. For complementary input check this (don't miss out on the Wang quote towards the very end): https://youtu.be/8H6vABTz6Wk?feature=shared41ReplySee all responsesMore from Cobus GreylingCobus GreylingFrom Handcrafted Workflows to AI Agents to Agentic WorkflowsThis evolution\\u200a‚Äî\\u200afrom handcrafted chatbot/RPA flows to AI-driven adaptive workflows\\u200a‚Äî\\u200ais transforming conversational AI, automation &‚Ä¶Feb 129067Cobus GreylingAI Agents are not Ready YetNo company wants to pour resources into developing software only to see it become irrelevant due to general advancements in AI‚Ä¶Mar 552512Cobus GreylingUsing LangChain With Model Context Protocol (MCP)The Model Context Protocol (MCP) is an open-source protocol developed by Anthropic, focusing on safe and interpretable Generative AI‚Ä¶Mar 104452Cobus GreylingModel Context Protocol (MCP)I would like to make a point regarding the Model Context Protocol (MCP)‚Ä¶Mar 112421See all from Cobus GreylingRecommended from MediumSean FalconerThe Future of AI Agents is Event-DrivenAI agents promise autonomy and adaptability. Event-driven architecture provides the backbone for these systems to scale and evolve.6d ago65117Vipra SinghAI Agents: Introduction (Part-1)Discover AI agents, their design, and real-world applications.Feb 21.4K29InEveryday AIbyManpreet SinghGoodbye RAG? Gemini 2.0 Flash Have Just Killed It!Alright!!!Feb 102.9K127Julio PessanDon‚Äôt Sell AI Agents, Sell AI Infrastructures Instead\\u200a‚Äî\\u200aThe Billion-Dollar OpportunityThe AI Mirage\\u200a‚Äî\\u200aAnd the Fortune Few See ComingMar 71K56InILLUMINATIONbyMr Tony MomohWhy Building Your AI Agent Could Be Your Most Valuable Investment in 2025‚ÄúA friend from Hong Kong told me about this last year.‚ÄùDec 30, 20243K112InCoding BeautybyTari IbabaThis new IDE from Google is an absolute game changerThis new IDE from Google is seriously revolutionary.Mar 111.5K94See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\n",
       " 'link': 'https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2',\n",
       " 'author': 'temp'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper = MediumCrawler()\n",
    "\n",
    "scraper.extract(\n",
    "    \"https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "DB = CLIENT.antonio\n",
    "PEOPLE = DB.people\n",
    "DOCUMENTS = DB.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67dade482ba62af1a5564a88'), 'platform': 'medium', 'content': {'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.', 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inHomeLibraryStoriesStatsDecoding MLHomeAbout¬∑Follow publicationBattle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. The hub for continuous learning on ML system design, ML engineering, MLOps, large language models (LLMs), and computer vision (CV).Follow publicationTop highlightFeaturedLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin¬∑FollowPublished inDecoding ML¬∑16 min read¬∑Mar 16, 20242.2K14ListenShare‚Üí the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the ‚ÄúLLM Twin: Building Your Production-Ready AI Replica‚Äù free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? ü´µ‚Üí No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish ‚Äî from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipeline crawls your digital data from various social media platforms. It cleans, normalizes and loads the data to a NoSQL DB through a series of ETL pipelines. Then, using the CDC pattern, it sends database changes to a queue.The feature pipeline consumes messages from a queue through a Bytewax streaming pipeline. It cleans, chunks, and embeds every message and loads it to a vector DB in real-time.The training pipeline creates a custom instruction dataset based on your digital data. Fine-tune an LLM using Unsloth, AWS SageMaker, and Comet ML's experiment tracker. Evaluate the LLMs using Opik and save the best model to the Hugging Face model registry.The inference pipeline loads and quantizes the fine-tuned LLM from the model registry to the AWS SageMaker REST API. Enhance the prompts using RAG. Monitor the LLM using Opik. Hook the LLM Twin to a Gradio UI.LLM Twin system architectureAlong the 4 microservices, you will learn to integrate 4 serverless tools:Comet ML as your experiment tracker;Qdrant as your vector DB;AWS SageMaker as your ML infrastructure;Opik as your prompt evaluation and monitoring tool.Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM and RAG systems using LLMOps best practices.Level: intermediatePrerequisites: basic knowledge of Python and ML.How will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.‚Üí To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you must know that we use several cloud tools that might generate additional costs.For example, AWS has a pay-as-you-go pricing plan. From our tests, it will cost you ~15$ to run the fine-tuning and inference pipelines.We will stick to their free version for the other serverless tools, such as Qdrant, Comet, and Opik.LessonsThe course is split into 12 lessons. Every Medium article will be its lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinYour Content is Gold: I Turned 3 Years of Blog Posts into an LLM TrainingI Replaced 1000 Lines of Polling Code with 50 Lines of CDC MagicSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG ‚Äî in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementTurning Raw Data Into Fine-Tuning Datasets8B Parameters, 1 GPU, No Problems: The Ultimate LLM Fine-tuning PipelineThe Engineer's Framework for LLM & RAG EvaluationBeyond Proof of Concept: Building RAG Systems That ScaleThe Ultimate Prompt Monitoring Pipeline[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Appsüîó Consider checking out the GitHub repository [1] and support us with a star Lesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component's code and learn how to implement and deploy it to AWS SageMaker.The LLM Twin's system architectureTable of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system designüîó Check out the code on GitHub [1] and support us with a star. What are you going to build?\"}}\n"
     ]
    }
   ],
   "source": [
    "for people in DOCUMENTS.find():\n",
    "    print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
