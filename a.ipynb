{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from loguru import logger\n",
    "import time\n",
    "import numpy as np\n",
    "from selenium import webdriver\n",
    "\n",
    "\n",
    "class MediumCrawler:\n",
    "    def __init__(self, scroll_limit: int = 5) -> None:\n",
    "        options = webdriver.ChromeOptions()\n",
    "\n",
    "        # options.add_argument(\"--no-sandbox\")\n",
    "        # options.add_argument(\"--headless=new\")\n",
    "        # options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        # options.add_argument(\"--log-level=3\")\n",
    "        # options.add_argument(\"--disable-popup-blocking\")\n",
    "        # options.add_argument(\"--disable-notifications\")\n",
    "        # options.add_argument(\"--disable-extensions\")\n",
    "        # options.add_argument(\"--disable-background-networking\")\n",
    "        # options.add_argument(\"--ignore-certificate-errors\")\n",
    "        # options.add_argument(f\"--user-data-dir={mkdtemp()}\")\n",
    "        # options.add_argument(f\"--data-path={mkdtemp()}\")\n",
    "        # options.add_argument(f\"--disk-cache-dir={mkdtemp()}\")\n",
    "        # options.add_argument(\"--remote-debugging-port=9226\")\n",
    "        # options.add_argument(r\"--profile-directory=Profile 2\")\n",
    "\n",
    "        self.scroll_limit = scroll_limit\n",
    "        self.driver = webdriver.Chrome(\n",
    "            options=options,\n",
    "        )\n",
    "\n",
    "    def scroll_page(self) -> None:\n",
    "        \"\"\"Scroll through the LinkedIn page based on the scroll limit.\"\"\"\n",
    "        current_scroll = 0\n",
    "        last_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        while True:\n",
    "            self.driver.execute_script(\n",
    "                \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "            )\n",
    "            time.sleep(max(1 + np.random.uniform(1), np.random.normal(2, 2)))\n",
    "            new_height = self.driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height or (\n",
    "                self.scroll_limit and current_scroll >= self.scroll_limit\n",
    "            ):\n",
    "                break\n",
    "            last_height = new_height\n",
    "            current_scroll += 1\n",
    "\n",
    "    def extract(self, link: str) -> None:\n",
    "        logger.info(f\"Starting scrapping Medium article: {link}\")\n",
    "\n",
    "        self.driver.get(link)\n",
    "        self.scroll_page()\n",
    "\n",
    "        soup = BeautifulSoup(self.driver.page_source, \"html.parser\")\n",
    "        title = soup.find_all(\"h1\", class_=\"pw-post-title\")\n",
    "        subtitle = soup.find_all(\"h2\", class_=\"pw-subtitle-paragraph\")\n",
    "\n",
    "        data = {\n",
    "            \"Title\": title[0].string if title else None,\n",
    "            \"Subtitle\": subtitle[0].string if subtitle else None,\n",
    "            \"Content\": soup.get_text(),\n",
    "        }\n",
    "\n",
    "        self.driver.close()\n",
    "        logger.info(f\"Successfully scraped and saved article: {link}\")\n",
    "\n",
    "        return {\"platfrom\": \"medium\", \"content\": data, \"link\": link, \"author\": \"temp\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 13:26:46.903\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m54\u001b[0m - \u001b[1mStarting scrapping Medium article: https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\u001b[0m\n",
      "\u001b[32m2025-03-19 13:26:55.516\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mextract\u001b[0m:\u001b[36m70\u001b[0m - \u001b[1mSuccessfully scraped and saved article: https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'platfrom': 'medium',\n",
       " 'content': {'Title': 'Why The Focus Has Shifted from AI Agents to Agentic Workflows',\n",
       "  'Subtitle': 'We find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. But…',\n",
       "  'Content': \"Why The Focus Has Shifted from AI Agents to Agentic Workflows | by Cobus Greyling | Feb, 2025 | MediumOpen in appSign upSign inWriteSign upSign inHomeLibraryStoriesStatsTop highlight1Why The Focus Has Shifted from AI Agents to Agentic WorkflowsWe find ourselves on a stairway from where Large Language Models were introduced to AI Agents with human like digital interactions. But…Cobus Greyling·Follow5 min read·Feb 5, 20251.5K45ListenShare…there has a shift when it comes to commercial implementations with focus moving form AI Agents in favor of Agentic Workflows/Data Synthesis.Why is the focus moving away from AI Agents (for now)?Companies like Salesforce and Service made hard pivots to AI Agents, however, the stark reality of AI Agents is that the technology is not where it should be in terms of accuracy.If one looks past the marketing hype, and the great prototypes and demos there are of AI Agents, their accuracy is not yet suited for production.The 𝗖𝗹𝗮𝘂𝗱𝗲 𝗔𝗜 𝗔𝗴𝗲𝗻𝘁 𝗖𝗼𝗺𝗽𝘂𝘁𝗲𝗿 𝗜𝗻𝘁𝗲𝗿𝗳𝗮𝗰𝗲 (𝗔𝗖𝗜) performance sits at 14% of that of human performance.The graph below from TheAgentFactory is an indication of where AI Agents sit in term of cost, steps and success rate. Notice how the success rate is around 20%.These figures are the stark reality of the current situation.SourceWith the recent release of the OpenAI Operator, computer use and web browser use accurate was reached of 30 to 50%, but this is still lagging below the 70%+ of human capability.SourceAnd added to this, there are interesting studies on how AI Agents with web browsing is susceptible and easy prey for attacks via nefarious pop-ups.There are two avenues for AI Agents to perform tasks like humans; the one is via a web browser (Webvoyager, OpenAI Operator, etc). The second is via the complete GUI of the OS (Anthropic).These approaches makes use of the GUI as the API for the AI Agents.Initial approaches looked at using individual APIs, but this is not practical due to the overhead of developing each and every API integration. Also, for many commercial applications there exists no API.Why the focus on Agentic WorkflowsEveryone is agreeing that modern knowledge work is broken, with various numbers being stated. One of the reports stated that workers spend 30% of their time searching for information.There is also a challenge for knowledge workers in answering complex questions and needing to synthesise information from various documents.Agentic Workflows (as shown in the image below) allows for reasoning and decomposing complex tasks into simpler sub-tasks and chaining these tasks together in a sequence.SourceBy executing this sequence, elements like observability, inspectability and discoverability are introduced.The synthesis of data will become increasingly important. Agentic Workflows for knowledge workers is one such example where work data and resources can be synthesised for the worker into one answer.Language Model providers are moving away from only offering the model, as such. But also extending into User Experience. Deep Research in ChatGPT is not a new model, but rather a new agentic capability within ChatGPT which conducts multi-step research on the internet for complex tasks. It accomplishes in tens of minutes what would take a human many hours.This is also a good example of how disparate sources of data is synthesised to answer a user question.I feel this is something LlamaIndex coined, the idea of Agentic RAG, where the notion is that synthesising data for an “audience of one” for a particular point in time will become important.In the coming months there will be immense focus on personal agentic workflows, information synthesis, something you could call desktop orchestration.Reasoning & Problem-SolvingModern AI models are increasingly integrating reasoning as a core feature, enabling them to tackle complex problems by breaking them down into manageable components.This shift is underpinned by an innovative approach that involves decomposing questions into smaller subsets, allowing the model to address each part systematically.By treating reasoning as an internal mechanism, these models can simulate human-like thought processes, enhancing their ability to provide accurate and nuanced responses.The decomposition strategy not only improves problem-solving efficiency but also fosters greater transparency in how conclusions are reached.As a result, users benefit from more interpretable outputs, bridging the gap between advanced computation and understandable decision-making.Initially users had to include reasoning traits in their prompt, instructing the model how to reason and decompose complex or compound tasks. And by giving examples via a few-show approach, for the model to emulate.In ClosingOrganisations must shift their focus from fixating on specific tools or trends — like those that once branded themselves as RAG companies, Prompt Engineering playgrounds and more, and instead prioritise solving real-world business challenges.The world is moving forward at an unprecedented pace, with new technologies emerging almost daily, each promising to revolutionise industries.But, the true measure of innovation lies not in mastering the latest technology but in applying these advancements to create tangible value.Whether it’s improving customer experiences, streamlining operations, or addressing societal needs, the question remains, how can we leverage technology to deliver meaningful solutions?By adopting this mindset, businesses can future-proof themselves and ensure they remain relevant amid ever-changing tides of progress.Chief Evangelist @ Kore.ai | I’m passionate about exploring the intersection of AI and language. From Language Models, AI Agents to Agentic Applications, Development Frameworks & Data-Centric Productivity Tools, I share insights and ideas on how these technologies are shaping the future.COBUS GREYLINGWhere AI Meets Language | Language Models, AI Agents, Agentic Applications, Development Frameworks & Data-Centric…www.cobusgreyling.comSign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for 5\\xa0$/monthAIArtificial IntelligenceMachine LearningLarge Language ModelsGenerative Ai Tools1.5K1.5K45FollowWritten by Cobus Greyling28K Followers·0 FollowingI’m passionate about exploring the intersection of AI & language. www.cobusgreyling.comFollowResponses (45)Write a responseWhat are your thoughts?CancelRespondAlso publish to my profileSDLC CorpheFeb 13The shift from AI Agents to Agentic Workflows reflects a crucial reality. AI needs more than just automation; it requires structured reasoning and reliability. While AI Agents show promise, their accuracy challenges highlight why businesses are…more21ReplyŞahin UtarFeb 12I expect a development towards self-integrating APIs where provider and consumer sides are deployed with agentic models that talk to each other and write and deploy api integration code (consume the other party's api, provide a web hook etc. based…more39ReplySten DiedenFeb 13Very interesting, sober and level headed in a context of otherwise incessant, hysterical hype. For complementary input check this (don't miss out on the Wang quote towards the very end): https://youtu.be/8H6vABTz6Wk?feature=shared41ReplySee all responsesMore from Cobus GreylingCobus GreylingFrom Handcrafted Workflows to AI Agents to Agentic WorkflowsThis evolution\\u200a—\\u200afrom handcrafted chatbot/RPA flows to AI-driven adaptive workflows\\u200a—\\u200ais transforming conversational AI, automation &…Feb 129067Cobus GreylingAI Agents are not Ready YetNo company wants to pour resources into developing software only to see it become irrelevant due to general advancements in AI…Mar 552512Cobus GreylingUsing LangChain With Model Context Protocol (MCP)The Model Context Protocol (MCP) is an open-source protocol developed by Anthropic, focusing on safe and interpretable Generative AI…Mar 104452Cobus GreylingModel Context Protocol (MCP)I would like to make a point regarding the Model Context Protocol (MCP)…Mar 112421See all from Cobus GreylingRecommended from MediumSean FalconerThe Future of AI Agents is Event-DrivenAI agents promise autonomy and adaptability. Event-driven architecture provides the backbone for these systems to scale and evolve.6d ago65117Vipra SinghAI Agents: Introduction (Part-1)Discover AI agents, their design, and real-world applications.Feb 21.4K29InEveryday AIbyManpreet SinghGoodbye RAG? Gemini 2.0 Flash Have Just Killed It!Alright!!!Feb 102.9K127Julio PessanDon’t Sell AI Agents, Sell AI Infrastructures Instead\\u200a—\\u200aThe Billion-Dollar OpportunityThe AI Mirage\\u200a—\\u200aAnd the Fortune Few See ComingMar 71K56InILLUMINATIONbyMr Tony MomohWhy Building Your AI Agent Could Be Your Most Valuable Investment in 2025“A friend from Hong Kong told me about this last year.”Dec 30, 20243K112InCoding BeautybyTari IbabaThis new IDE from Google is an absolute game changerThis new IDE from Google is seriously revolutionary.Mar 111.5K94See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\"},\n",
       " 'link': 'https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2',\n",
       " 'author': 'temp'}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraper = MediumCrawler()\n",
    "\n",
    "scraper.extract(\"https://medium.com/@cobusgreyling/why-the-focus-has-shifted-from-ai-agents-to-agentic-workflows-51e4078d03c2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "CLIENT: MongoClient = MongoClient(\"localhost\", 27017)\n",
    "DB = CLIENT.antonio\n",
    "PEOPLE = DB.people\n",
    "DOCUMENTS = DB.documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('67dacc95c9ddc975da97de94'), 'platfrom': 'medium', 'content': {'Title': 'An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM Twin', 'Subtitle': 'From data gathering to productionizing LLMs using LLMOps good practices.', 'Content': \"End-to-End Framework for Production-Ready LLMs | Decoding MLOpen in appSign upSign inWriteSign upSign inHomeLibraryStoriesStatsDecoding MLHomeAbout·Follow publicationBattle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. The hub for continuous learning on ML system design, ML engineering, MLOps, large language models (LLMs), and computer vision (CV).Follow publicationTop highlightFeaturedLLM Twin Course: Building Your Production-Ready AI ReplicaAn End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinFrom data gathering to productionizing LLMs using LLMOps good practices.Paul Iusztin·FollowPublished inDecoding ML·16 min read·Mar 16, 20242.2K14ListenShare→ the 1st out of 12 lessons of the LLM Twin free courseWhat is your LLM Twin? It is an AI character that writes like yourself by incorporating your style, personality and voice into an LLM.Image by DALL-EWhy is this course different?By finishing the “LLM Twin: Building Your Production-Ready AI Replica” free course, you will learn how to design, train, and deploy a production-ready LLM twin of yourself powered by LLMs, vector DBs, and LLMOps good practices.Why should you care? 🫵→ No more isolated scripts or Notebooks! Learn production ML by building and deploying an end-to-end production-grade LLM system.What will you learn to build by the end of this course?You will learn how to architect and build a real-world LLM system from start to finish — from data collection to deployment.You will also learn to leverage MLOps best practices, such as experiment trackers, model registries, prompt monitoring, and versioning.The end goal? Build and deploy your own LLM twin.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipeline crawls your digital data from various social media platforms. It cleans, normalizes and loads the data to a NoSQL DB through a series of ETL pipelines. Then, using the CDC pattern, it sends database changes to a queue.The feature pipeline consumes messages from a queue through a Bytewax streaming pipeline. It cleans, chunks, and embeds every message and loads it to a vector DB in real-time.The training pipeline creates a custom instruction dataset based on your digital data. Fine-tune an LLM using Unsloth, AWS SageMaker, and Comet ML’s experiment tracker. Evaluate the LLMs using Opik and save the best model to the Hugging Face model registry.The inference pipeline loads and quantizes the fine-tuned LLM from the model registry to the AWS SageMaker REST API. Enhance the prompts using RAG. Monitor the LLM using Opik. Hook the LLM Twin to a Gradio UI.LLM Twin system architectureAlong the 4 microservices, you will learn to integrate 4 serverless tools:Comet ML as your experiment tracker;Qdrant as your vector DB;AWS SageMaker as your ML infrastructure;Opik as your prompt evaluation and monitoring tool.Who is this for?Audience: MLE, DE, DS, or SWE who want to learn to engineer production-ready LLM and RAG systems using LLMOps best practices.Level: intermediatePrerequisites: basic knowledge of Python and ML.How will you learn?The course contains 10 hands-on written lessons and the open-source code you can access on GitHub, showing how to build an end-to-end LLM system.Also, it includes 2 bonus lessons on how to improve the RAG system.You can read everything at your own pace.→ To get the most out of this course, we encourage you to clone and run the repository while you cover the lessons.Costs?The articles and code are completely free. They will always remain free.But if you plan to run the code while reading it, you must know that we use several cloud tools that might generate additional costs.For example, AWS has a pay-as-you-go pricing plan. From our tests, it will cost you ~15$ to run the fine-tuning and inference pipelines.We will stick to their free version for the other serverless tools, such as Qdrant, Comet, and Opik.LessonsThe course is split into 12 lessons. Every Medium article will be its lesson:An End-to-End Framework for Production-Ready LLM Systems by Building Your LLM TwinYour Content is Gold: I Turned 3 Years of Blog Posts into an LLM TrainingI Replaced 1000 Lines of Polling Code with 50 Lines of CDC MagicSOTA Python Streaming Pipelines for Fine-tuning LLMs and RAG — in Real-Time!The 4 Advanced RAG Algorithms You Must Know to ImplementTurning Raw Data Into Fine-Tuning Datasets8B Parameters, 1 GPU, No Problems: The Ultimate LLM Fine-tuning PipelineThe Engineer’s Framework for LLM & RAG EvaluationBeyond Proof of Concept: Building RAG Systems That ScaleThe Ultimate Prompt Monitoring Pipeline[Bonus] Build a scalable RAG ingestion pipeline using 74.3% less code[Bonus] Build Multi-Index Advanced RAG Apps🔗 Consider checking out the GitHub repository [1] and support us with a ⭐️Lesson 1: End-to-end framework for production-ready LLM systemsIn the first lesson, we will present the project you will build during the course: your production-ready LLM Twin/AI replica.Afterward, we will explain what the 3-pipeline design is and how it is applied to a standard ML system.Ultimately, we will dig into the LLM project system design.We will present all our architectural decisions regarding the design of the data collection pipeline for social media data and how we applied the 3-pipeline architecture to our LLM microservices.In the following lessons, we will examine each component’s code and learn how to implement and deploy it to AWS SageMaker.The LLM Twin’s system architectureTable of ContentsWhat are you going to build? The LLM twin conceptThe 3-pipeline architectureLLM twin system design🔗 Check out the code on GitHub [1] and support us with a ⭐️1. What are you going to build? The LLM twin conceptThe outcome of this course is to learn to build your own AI replica. We will use an LLM to do that, hence the name of the course: LLM Twin: Building Your Production-Ready AI Replica.But what is an LLM twin?Shortly, your LLM twin will be an AI character who writes like you, using your writing style and personality.It will not be you. It will be your writing copycat.More concretely, you will build an AI replica that writes social media posts or technical articles (like this one) using your own voice.Why not directly use ChatGPT? You may ask…When trying to generate an article or post using an LLM, the results tend to:be very generic and unarticulated,contain misinformation (due to hallucination),require tedious prompting to achieve the desired result.But here is what we are going to do to fix that ↓↓↓First, we will fine-tune an LLM on your digital data gathered from LinkedIn, Medium, Substack and GitHub.By doing so, the LLM will align with your writing style and online personality. It will teach the LLM to talk like the online version of yourself.Have you seen the universe of AI characters Meta released in 2024 in the Messenger app? If not, you can learn more about it here [2].To some extent, that is what we are going to build.But in our use case, we will focus on an LLM twin who writes social media posts or articles that reflect and articulate your voice.For example, we can ask your LLM twin to write a LinkedIn post about LLMs. Instead of writing some generic and unarticulated post about LLMs (e.g., what ChatGPT will do), it will use your voice and style.Secondly, we will give the LLM access to a vector DB to access external information to avoid hallucinating. Thus, we will force the LLM to write only based on concrete data.Ultimately, in addition to accessing the vector DB for information, you can provide external links that will act as the building block of the generation process.For example, we can modify the example above to: “Write me a 1000-word LinkedIn post about LLMs based on the article from this link: [URL].”Excited? Let’s get started 🔥2. The 3-pipeline architectureWe all know how messy ML systems can get. That is where the 3-pipeline architecture kicks in.The 3-pipeline design brings structure and modularity to your ML system while improving your MLOps processes.ProblemDespite advances in MLOps tooling, transitioning from prototype to production remains challenging.In 2022, only 54% of the models get into production. Auch.So what happens?Maybe the first things that come to your mind are:the model is not mature enoughsecurity risks (e.g., data privacy)not enough dataTo some extent, these are true.But the reality is that in many scenarios……the architecture of the ML system is built with research in mind, or the ML system becomes a massive monolith that is extremely hard to refactor from offline to online.So, good SWE processes and a well-defined architecture are as crucial as using suitable tools and models with high accuracy.Solution→ The 3-pipeline architectureLet’s understand what the 3-pipeline design is.It is a mental map that helps you simplify the development process and split your monolithic ML pipeline into 3 components:1. the feature pipeline2. the training pipeline3. the inference pipeline…also known as the Feature/Training/Inference (FTI) architecture.#1. The feature pipeline transforms your data into features & labels, which are stored and versioned in a feature store. The feature store will act as the central repository of your features. That means that features can be accessed and shared only through the feature store.#2. The training pipeline ingests a specific version of the features & labels from the feature store and outputs the trained model weights, which are stored and versioned inside a model registry. The models will be accessed and shared only through the model registry.#3. The inference pipeline uses a given version of the features from the feature store and downloads a specific version of the model from the model registry. Its final goal is to output the predictions to a client.The FTI architectureThis is why the 3-pipeline design is so beautiful:- it is intuitive- it brings structure, as on a higher level, all ML systems can be reduced to these 3 components- it defines a transparent interface between the 3 components, making it easier for multiple teams to collaborate- the ML system has been built with modularity in mind since the beginning- the 3 components can easily be divided between multiple teams (if necessary)- every component can use the best stack of technologies available for the job- every component can be deployed, scaled, and monitored independently- the feature pipeline can easily be either batch, streaming or bothBut the most important benefit is that……by following this pattern, you know 100% that your ML model will move out of your Notebooks into production.↳ If you want to learn more about the 3-pipeline design, I recommend this excellent article [3] written by Jim Dowling, one of the creators of the FTI architecture.3. LLM Twin System designLet’s understand how to apply the 3-pipeline architecture to our LLM system.The architecture of the LLM twin is split into 4 Python microservices:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineThe LLM Twin’s system architectureAs you can see, the data collection pipeline doesn’t follow the 3-pipeline design, which is true.It represents the data pipeline that sits before the ML system.The data engineering team usually implements it, and its scope is to gather, clean, normalize and store the data required to build dashboards or ML models.But let’s say you are part of a small team and have to build everything yourself, from data gathering to model deployment.Thus, we will show you how the data pipeline nicely fits and interacts with the FTI architecture.Now, let’s zoom in on each component to understand how they work individually and interact with each other. ↓↓↓3.1. The data collection pipelineIts scope is to crawl data for a given user from:Medium (articles)Substack (articles)LinkedIn (posts)GitHub (code)As every platform is unique, we implemented a different Extract Transform Load (ETL) pipeline for each website.🔗 1-min read on ETL pipelines [4]However, the baseline steps are the same for each platform.Thus, for each ETL pipeline, we can abstract away the following baseline steps:log in using your credentialsuse selenium to crawl your profileuse BeatifulSoup to parse the HTMLclean & normalize the extracted HTMLsave the normalized (but still raw) data to Mongo DBImportant note: We are crawling only our data, as most platforms do not allow us to access other people’s data due to privacy issues. But this is perfect for us, as to build our LLM twin, we need only our own digital data.Why Mongo DB?We wanted a NoSQL database that quickly allows us to store unstructured data (aka text).How will the data pipeline communicate with the feature pipeline?We will use the Change Data Capture (CDC) pattern to inform the feature pipeline of any change on our Mongo DB.🔗 1-min read on the CDC pattern [5]To explain the CDC briefly, a watcher listens 24/7 for any CRUD operation that happens to the Mongo DB.The watcher will issue an event informing us what has been modified. We will add that event to a RabbitMQ queue.The feature pipeline will constantly listen to the queue, process the messages, and add them to the Qdrant vector DB.For example, when we write a new document to the Mongo DB, the watcher creates a new event. The event is added to the RabbitMQ queue; ultimately, the feature pipeline consumes and processes it.Doing this ensures that the Mongo DB and vector DB are constantly in sync.With the CDC technique, we transition from a batch ETL pipeline (our data pipeline) to a streaming pipeline (our feature pipeline).Using the CDC pattern, we avoid implementing a complex batch pipeline to compute the difference between the Mongo DB and vector DB. This approach can quickly get very slow when working with big data.Where will the data pipeline be deployed?The data collection pipeline and RabbitMQ service will be deployed to AWS. We will also use the freemium serverless version of Mongo DB.3.2. The feature pipelineThe feature pipeline is implemented using Bytewax (a Rust streaming engine with a Python interface). Thus, in our specific use case, we will also refer to it as a streaming ingestion pipeline.It is an entirely different service than the data collection pipeline.How does it communicate with the data pipeline?As explained above, the feature pipeline communicates with the data pipeline through a RabbitMQ queue.Currently, the streaming pipeline doesn’t care how the data is generated or where it comes from.It knows it has to listen to a given queue, consume messages from there and process them.By doing so, we decouple the two components entirely. In the future, we can easily add messages from multiple sources to the queue, and the streaming pipeline will know how to process them. The only rule is that the messages in the queue should always respect the same structure/interface.What is the scope of the feature pipeline?It represents the ingestion component of the RAG system.It will take the raw data passed through the queue and:clean the data;chunk it;embed it using the embedding models from Superlinked;load it to the Qdrant vector DB.Every type of data (post, article, code) will be processed independently through its own set of classes.Even though all of them are text-based, we must clean, chunk and embed them using different strategies, as every type of data has its own particularities.What data will be stored?The training pipeline will have access only to the feature store, which, in our case, is represented by the Qdrant vector DB.Note that a vector DB can also be used as a NoSQL DB.With these 2 things in mind, we will store in Qdrant 2 snapshots of our data:1. The cleaned data (without using vectors as indexes — store them in a NoSQL fashion).2. The cleaned, chunked, and embedded data (leveraging the vector indexes of Qdrant)The training pipeline needs access to the data in both formats as we want to fine-tune the LLM on standard and augmented prompts.With the cleaned data, we will create the prompts and answers.With the chunked data, we will augment the prompts (aka RAG).Why implement a streaming pipeline instead of a batch pipeline?There are 2 main reasons.The first one is that, coupled with the CDC pattern, it is the most efficient way to sync two DBs between each other. Otherwise, you would have to implement batch polling or pushing techniques that aren’t scalable when working with big data.Using CDC + a streaming pipeline, you process only the changes to the source DB without any overhead.The second reason is that by doing so, your source and vector DB will always be in sync. Thus, you will always have access to the latest data when doing RAG.Why Bytewax?Bytewax is a streaming engine built in Rust that exposes a Python interface. We use Bytewax because it combines Rust’s impressive speed and reliability with the ease of use and ecosystem of Python. It is incredibly light, powerful, and easy for a Python developer.Where will the feature pipeline be deployed?The feature pipeline will be deployed to AWS. We will also use the freemium serverless version of Qdrant.3.3. The training pipelineHow do we have access to the training features?As highlighted in section 3.2, all the training data will be accessed from the feature store. In our case, the feature store is the Qdrant vector DB that contains:the cleaned digital data from which we will create prompts & answers;we will use the chunked & embedded data for RAG to augment the cleaned data.We will implement a different vector DB retrieval client for each of our main types of data (posts, articles, code).We must do this separation because we must preprocess each type differently before querying the vector DB, as each type has unique properties.Also, we will add custom behavior for each client based on what we want to query from the vector DB. But more on this in its dedicated lesson.What will the training pipeline do?The training pipeline contains a data-to-prompt layer that will preprocess the data retrieved from the vector DB into prompts.It will also contain an LLM fine-tuning module that inputs a HuggingFace dataset and uses QLoRA to fine-tune a given LLM (e.g., Mistral). By using HuggingFace, we can easily switch between different LLMs so we won’t focus too much on any specific LLM.All the experiments will be logged into Comet ML’s experiment tracker.We will use a bigger LLM (e.g., GPT4) to evaluate the results of our fine-tuned LLM. These results will be logged into Comet’s experiment tracker.Where will the production candidate LLM be stored?We will compare multiple experiments, pick the best one, and issue an LLM production candidate for the model registry.After, we will inspect the LLM production candidate manually using Comet’s prompt monitoring dashboard. If this final manual check passes, we will flag the LLM from the model registry as accepted.A CI/CD pipeline will trigger and deploy the new LLM version to the inference pipeline.Where will the training pipeline be deployed?The training pipeline will be deployed to AWS SageMaker.AWS SageMaker is a solution for training and deploying ML models. It makes scaling your operation easy while you can focus on building.Also, we will use the freemium version of Comet ML for the following:experiment tracker;model registry;prompt monitoring.3.4. The inference pipelineThe inference pipeline is the final component of the LLM system. It is the one the clients will interact with.It will be wrapped under a REST API. The clients can call it through HTTP requests, similar to your experience with ChatGPT or similar tools.How do we access the features?To access the feature store, we will use the same Qdrant vector DB retrieval clients as in the training pipeline.In this case, we will need the feature store to access the chunked data to do RAG.How do we access the fine-tuned LLM?The fine-tuned LLM will always be downloaded from the model registry based on its tag (e.g., accepted) and version (e.g., v1.0.2, latest, etc.).How will the fine-tuned LLM be loaded?Here we are in the inference world.Thus, we want to optimize the LLM's speed and memory consumption as much as possible. That is why, after downloading the LLM from the model registry, we will quantize it.What are the components of the inference pipeline?The first one is the retrieval client used to access the vector DB to do RAG. This is the same module as the one used in the training pipeline.After we have a query to prompt the layer, that will map the prompt and retrieved documents from Qdrant into a prompt.After the LLM generates its answer, we will log it to Comet’s prompt monitoring dashboard and return it to the clients.For example, the client will request the inference pipeline to:“Write a 1000-word LinkedIn post about LLMs,” and the inference pipeline will go through all the steps above to return the generated post.Where will the inference pipeline be deployed?The inference pipeline will be deployed to AWS SageMaker.AWS SageMaker also offers autoscaling solutions and a nice dashboard to monitor all the production environment resources.ConclusionThis is the 1st article of the LLM Twin: Building Your Production-Ready AI Replica free course.In this lesson, we presented what you will build during the course.After we briefly discussed how to design ML systems using the 3-pipeline design.Ultimately, we went through the system design of the course and presented the architecture of each microservice and how they interact with each other:The data collection pipelineThe feature pipelineThe training pipelineThe inference pipelineIn Lesson 2, we will dive deeper into the data collection pipeline, learn how to implement crawlers for various social media platforms, clean the gathered data, and store it in a MongoDB NoSQL database.🔗 Consider checking out the GitHub repository [1] and support us with a ⭐️Our LLM Engineer’s Handbook inspired the open-source LLM Twin course.Consider supporting our work by getting our book to learn a complete framework for building and deploying production LLM & RAG systems — from data to deployment.Perfect for practitioners who want both theory and hands-on expertise by connecting the dots between DE, research, MLE and MLOps:→ Buy the LLM Engineer’s Handbook (on Amazon or Packt)LLM Engineer’s Handbook CoverEnjoyed This Article?Join Decoding ML for battle-tested content on designing, coding, and deploying production-grade LLM, RecSys & MLOps systems. Every week, a new project ↓Decoding ML Newsletter | Paul Iusztin | SubstackJoin for battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. Every week. For…decodingml.substack.comReferencesLiterature[1] Your LLM Twin Course — GitHub Repository (2024), Decoding ML GitHub Organization[2] Introducing new AI experiences from Meta (2023), Meta[3] Jim Dowling, From MLOps to ML Systems with Feature/Training/Inference Pipelines (2023), Hopsworks[4] Extract Transform Load (ETL), Databricks Glossary[5] Daniel Svonava and Paolo Perrone, Understanding the different Data Modality / Types (2023), SuperlinkedImagesIf not otherwise stated, all images are created by the author.Sign up to discover human stories that deepen your understanding of the world.FreeDistraction-free reading. No ads.Organize your knowledge with lists and highlights.Tell your story. Find your audience.Sign up for freeMembershipRead member-only storiesSupport writers you read mostEarn money for your writingListen to audio narrationsRead offline with the Medium appTry for 5\\xa0$/monthGenerative AiLarge Language ModelsMlopsArtificial IntelligenceMachine Learning2.2K2.2K14FollowPublished in Decoding ML1.5K Followers·Last published\\xa0Nov 30, 2024Battle-tested content on designing, coding, and deploying production-grade ML & MLOps systems. The hub for continuous learning on ML system design, ML engineering, MLOps, large language models (LLMs), and computer vision (CV).FollowFollowWritten by Paul Iusztin5.5K Followers·244 FollowingSenior AI/ML Engineer • Founder @ Decoding ML ~ Articles, code, and courses about building production-grade AI systems.FollowResponses (14)Write a responseWhat are your thoughts?CancelRespondAlso publish to my profileAdijsadheMar 20, 2024\\xa0(edited)What if I want to feed pdfs? Can you also add an ETL to extract pdfs in a clean and clear way without losing information present in the tables, code, equation etc...?182 repliesReplyM K Pavan KumarMar 19, 2024This is awesome!!171 replyReplyDaniel GarcíaMar 19, 2024Great content!!131 replyReplySee all responsesMore from Paul Iusztin and Decoding MLInData Science CollectivebyPaul IusztinBuilding a TikTok-like recommenderScaling a personalized recommender to millions of items in real-timeMar 82042InDecoding MLbyPaul IusztinThe 4 Advanced RAG Algorithms You Must Know to ImplementImplement from scratch 4 advanced RAG methods to optimize your retrieval and post-retrieval algorithmMay 4, 20241.8K15InDecoding MLbyPaul Iusztin8B Parameters, 1 GPU, No Problems: The Ultimate LLM Fine-tuning PipelineMaster production-ready fine-tuning with AWS SageMaker, Unsloth, and MLOps best practicesNov 18, 2024145InDecoding MLbyPaul IusztinYour Content is Gold: I Turned 3 Years of Blog Posts into an LLM TrainingA practical guide to building custom instruction datasets for fine-tuning LLMsNov 18, 202499See all from Paul IusztinSee all from Decoding MLRecommended from MediumVipra SinghAI Agent: Workflow vs Agent (Part-5)Discover AI agents, their design, and real-world applications.5d ago42513InTDS ArchivebyDominik Polzer17 (Advanced) RAG Techniques to Turn Your LLM App Prototype into a Production-Ready SolutionA collection of RAG techniques to help you develop your RAG app into something robust that will lastJun 26, 20242.9K31Julio PessanDon’t Sell AI Agents, Sell AI Infrastructures Instead\\u200a—\\u200aThe Billion-Dollar OpportunityThe AI Mirage\\u200a—\\u200aAnd the Fortune Few See ComingMar 71K56InTowards AIbyMaxime JabarianBuild Your LLM Engineer Portfolio: A 3-Month RoadmapA step-by-step guide to designing, refining, and showcasing a portfolio that kickstarts your career.Dec 15, 20241.8K18InStackademicbyShanojHow We Built LLM Infrastructure That Actually Works\\u200a—\\u200aAnd What We LearnedA Data Engineer’s Complete Roadmap: From Napkin Diagrams to Production-Ready Architecture4d ago101InLevel Up CodingbyFareed KhanTesting 18 RAG Techniques to Find the Bestcrag, HyDE, fusion and more!Mar 1275513See more recommendationsHelpStatusAboutCareersPressBlogPrivacyTermsText to speechTeams\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTo make Medium work, we log user data. By using Medium, you agree to our Privacy Policy, including cookie policy.\"}, 'link': 'https://medium.com/decodingml/an-end-to-end-framework-for-production-ready-llm-systems-by-building-your-llm-twin-2cc6bb01141f', 'author': 'Paul Iusztin'}\n"
     ]
    }
   ],
   "source": [
    "for people in DOCUMENTS.find():\n",
    "    print(people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
